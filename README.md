# ðŸŽ™ï¸ Paper â†’ Podcast

Transform dense academic arXiv papers into engaging two-host conversational podcasts â€” complete with a readable transcript and optional multi-voice audio.

## Features

- **Structured PDF parsing** â€” extracts title, authors, abstract, methodology, results, discussion, and conclusion from any arXiv paper using PyMuPDF.
- **LaTeX â†’ spoken English** â€” converts math notation into natural language before dialogue generation (e.g., `x^2` â†’ "x squared").
- **Multi-stage dialogue generation** â€” not a single monolithic prompt; generates a summary first, then section-by-section dialogue with distinct Host/Expert personas.
- **Three interchangeable LLM backends** â€” Groq (default, ultra-fast), OpenAI, Anthropic, or fully offline via Ollama (Mistral-7B). Swap with one config variable.
- **Script post-processing** â€” normalises speaker labels, strips residual artefacts, injects sparse conversational fillers, and adds timestamps.
- **Multi-voice TTS** â€” Groq TTS with PlayAI voices (default), edge-tts (free Microsoft voices), or Coqui TTS (fully offline). Generates per-turn audio and concatenates with natural pacing.
- **Clean Streamlit UI** â€” progress bar, split transcript/audio view, download buttons.

## Project Structure

```
fysem2/
â”œâ”€â”€ app.py                  # Streamlit interface
â”œâ”€â”€ config.py               # Central configuration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ paper_parser.py     # PDF download & structured extraction
â”‚   â”œâ”€â”€ latex_to_speech.py  # LaTeX â†’ spoken English converter
â”‚   â”œâ”€â”€ llm_interface.py    # Unified LLM abstraction layer
â”‚   â”œâ”€â”€ dialogue_generator.py  # Multi-stage dialogue pipeline
â”‚   â”œâ”€â”€ post_processor.py   # Script cleaning & polishing
â”‚   â”œâ”€â”€ tts_engine.py       # Multi-voice audio generation
â”‚   â””â”€â”€ pipeline.py         # End-to-end orchestrator
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ templates.py        # System prompts, personas, few-shot examples
â””â”€â”€ output/                 # Generated transcripts & audio files
```

## Quick Start

### 1. Clone & set up the virtual environment

```bash
cd fysem2
python -m venv venv

# Windows
.\venv\Scripts\activate

# macOS / Linux
source venv/bin/activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

> **Note:** You also need **ffmpeg** installed on your system for pydub audio processing.
> - Windows: `choco install ffmpeg` or download from https://ffmpeg.org
> - macOS: `brew install ffmpeg`
> - Linux: `sudo apt install ffmpeg`

### 3. Choose your LLM backend

#### Option A â€” Groq (fast, free tier, recommended)

1. Get a free API key at https://console.groq.com
2. Set it:
   ```bash
   set GROQ_API_KEY=gsk_...        # Windows
   export GROQ_API_KEY=gsk_...     # macOS/Linux
   ```
3. No extra config needed â€” Groq is the default backend for both LLM **and** TTS.

#### Option B â€” Ollama (free, fully offline)

1. Install Ollama: https://ollama.com/download
2. Pull a model:
   ```bash
   ollama pull mistral
   ```
3. Start the server:
   ```bash
   ollama serve
   ```
4. Set backend:
   ```bash
   set LLM_BACKEND=ollama
   ```

#### Option C â€” OpenAI

```bash
set OPENAI_API_KEY=sk-...       # Windows
export OPENAI_API_KEY=sk-...    # macOS/Linux
set LLM_BACKEND=openai
```

#### Option D â€” Anthropic

```bash
set ANTHROPIC_API_KEY=sk-ant-...
set LLM_BACKEND=anthropic
```

### 4. Run the app

```bash
streamlit run app.py
```

Open http://localhost:8501 in your browser, paste an arXiv URL, and click **Generate Podcast**.

## Configuration

All settings are in `config.py`:

| Variable | Default | Description |
|---|---|---|
| `LLM_BACKEND` | `"groq"` | `"groq"`, `"openai"`, `"anthropic"`, or `"ollama"` |
| `GROQ_API_KEY` | `""` | Your Groq API key (used for both LLM and TTS) |
| `GROQ_MODEL` | `"llama-3.3-70b-versatile"` | Groq LLM model |
| `TTS_ENGINE` | `"groq"` | `"groq"`, `"edge"`, or `"coqui"` |
| `GROQ_VOICE_HOST` | `"tara"` | Host voice for Groq TTS |
| `GROQ_VOICE_EXPERT` | `"dan"` | Expert voice for Groq TTS |
| `OLLAMA_MODEL` | `"mistral"` | Any model pulled into Ollama |
| `EDGE_VOICE_HOST` | `"en-US-JennyNeural"` | Host voice for edge-tts |
| `EDGE_VOICE_EXPERT` | `"en-US-GuyNeural"` | Expert voice for edge-tts |
| `SILENCE_BETWEEN_TURNS_MS` | `600` | Pause between speaker turns |
| `LLM_TEMPERATURE` | `0.7` | Creativity of dialogue generation |

## System Requirements

- Python 3.10+
- 8â€“16 GB RAM (Ollama models need ~4â€“8 GB)
- ffmpeg on PATH
- Internet connection for arXiv downloads and edge-tts (not needed for Ollama + Coqui)

## Module Overview

| Module | Responsibility |
|---|---|
| `paper_parser.py` | Downloads the PDF, extracts text with PyMuPDF, splits into structured sections, strips citations and figure references |
| `latex_to_speech.py` | Regex + lookup-table converter that transforms LaTeX into spoken English (handles fractions, superscripts, Greek letters, nested expressions) |
| `llm_interface.py` | Unified `query_llm()` function that dispatches to OpenAI / Anthropic / Ollama behind a common interface |
| `dialogue_generator.py` | Multi-stage pipeline: summary â†’ per-section dialogue â†’ takeaway, with persona prompts and few-shot examples |
| `post_processor.py` | Normalises speaker labels, removes artefacts, injects fillers, adds timestamps and segment markers |
| `tts_engine.py` | Generates per-turn audio clips (edge-tts or Coqui VITS), concatenates with silence gaps via pydub |
| `pipeline.py` | Orchestrates all stages with progress reporting |
| `templates.py` | Contains all prompt templates, persona definitions, and few-shot examples |

## License

MIT
